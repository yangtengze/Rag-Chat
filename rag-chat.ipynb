{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM generate Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler\n",
    "from sparkai.core.messages import ChatMessage\n",
    "\n",
    "#星火认知大模型Spark Max的URL值，其他版本大模型URL值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_URL = 'wss://spark-api.xf-yun.com/v3.5/chat'\n",
    "#星火认知大模型调用秘钥信息，请前往讯飞开放平台控制台（https://console.xfyun.cn/services/bm35）查看\n",
    "SPARKAI_APP_ID = ''\n",
    "SPARKAI_API_SECRET = ''\n",
    "SPARKAI_API_KEY = ''\n",
    "#星火认知大模型Spark Max的domain值，其他版本大模型domain值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_DOMAIN = 'generalv3.5'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = ChatSparkLLM(\n",
    "        spark_api_url=SPARKAI_URL,\n",
    "        spark_app_id=SPARKAI_APP_ID,\n",
    "        spark_api_key=SPARKAI_API_KEY,\n",
    "        spark_api_secret=SPARKAI_API_SECRET,\n",
    "        spark_llm_domain=SPARKAI_DOMAIN,\n",
    "        streaming=False,\n",
    "    )\n",
    "    messages = [ChatMessage(\n",
    "        role=\"user\",\n",
    "        content='我最近一直流鼻涕，打喷嚏，头还痛，浑身乏力，这是怎么回事，怎么办啊？'\n",
    "    )]\n",
    "    handler = ChunkPrintHandler()\n",
    "    a = spark.generate([messages], callbacks=[handler])\n",
    "    print(a.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG based on es search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## text trunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "csv_file = './data.csv'\n",
    "documents = []\n",
    "\n",
    "with open(csv_file, mode='r', encoding='utf-8') as file:\n",
    "    csv_reader = csv.DictReader(file)\n",
    "    for row in csv_reader:\n",
    "        documents.append(row)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch,helpers\n",
    "\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "index_name = \"my_index\"\n",
    "for i, doc in enumerate(documents):\n",
    "    rs = es.index(index=index_name, id=i, body=doc)\n",
    "    print(rs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = {\n",
    "    \"query\": {\n",
    "        \"match\": {\n",
    "            \"query\": \"我最近一直流鼻涕，打喷嚏，头还痛，浑身乏力，这是怎么回事，怎么办啊？\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "response = es.search(index=index_name, body=search_query)\n",
    "print(response)\n",
    "print(\"搜索结果：\")\n",
    "for hit in response['hits']['hits']:\n",
    "    print(f\"Query: {hit['_source']['query']}\")\n",
    "    print(f\"Answer: {hit['_source']['answer']}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "for hit in response['hits']['hits']:\n",
    "    docs.append(hit['_source']['query'] + '\\n' + hit['_source']['answer'])\n",
    "print(docs)\n",
    "\n",
    "\n",
    "query = search_query['query']['match']['query']\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"\n",
    "请根据我下面的文档内容，回答我提出的问题：\n",
    "文档内容：{docs}\n",
    "问题：{query}\n",
    "一律用中文回答\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler\n",
    "from sparkai.core.messages import ChatMessage\n",
    "\n",
    "#星火认知大模型Spark Max的URL值，其他版本大模型URL值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_URL = 'wss://spark-api.xf-yun.com/v3.5/chat'\n",
    "#星火认知大模型调用秘钥信息，请前往讯飞开放平台控制台（https://console.xfyun.cn/services/bm35）查看\n",
    "\n",
    "\n",
    "SPARKAI_APP_ID = ''\n",
    "SPARKAI_API_SECRET = ''\n",
    "SPARKAI_API_KEY = ''\n",
    "\n",
    "\n",
    "#星火认知大模型Spark Max的domain值，其他版本大模型domain值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_DOMAIN = 'generalv3.5'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = ChatSparkLLM(\n",
    "        spark_api_url=SPARKAI_URL,\n",
    "        spark_app_id=SPARKAI_APP_ID,\n",
    "        spark_api_key=SPARKAI_API_KEY,\n",
    "        spark_api_secret=SPARKAI_API_SECRET,\n",
    "        spark_llm_domain=SPARKAI_DOMAIN,\n",
    "        streaming=False,\n",
    "    )\n",
    "    messages = [ChatMessage(\n",
    "        role=\"user\",\n",
    "        content= prompt\n",
    "    )]\n",
    "    handler = ChunkPrintHandler()\n",
    "    a = spark.generate([messages], callbacks=[handler])\n",
    "    print(a.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG based on text2vec & cos_smi "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from text2vec import SentenceModel\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "model = SentenceModel('./text2vec-base-multilingual')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentense2vec(sentense):\n",
    "    embedding = model.encode(sentense)\n",
    "    return embedding\n",
    "\n",
    "def documents_vec(file_path):\n",
    "    documents = []\n",
    "    with open(file_path,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    head = lines[0]\n",
    "    lines = lines[1:]\n",
    "    \n",
    "    print('documents to vec')\n",
    "    for line in tqdm(lines):\n",
    "        embedding = sentense2vec(line)\n",
    "        documents.append(embedding)\n",
    "    return lines,head,documents\n",
    "\n",
    "original_docs,head,documents = documents_vec(\"./data.csv\")\n",
    "print(head)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## docs generate Based on cos_smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import numpy as np\n",
    "\n",
    "def generate_docs(query,documents,top_k):\n",
    "    query = model.encode(query)\n",
    "    query = np.array(query).reshape(1,-1)\n",
    "    cos_sim = []\n",
    "    for i,document in enumerate(documents):\n",
    "        document = np.array(document).reshape(1,-1)\n",
    "        cos_sim.append(\n",
    "            { i: cosine_similarity(query,document)[0,0]}\n",
    "        )\n",
    "    cos_sim.sort(key=lambda x: list(x.values())[0],reverse=True)\n",
    "    top_k_array = np.array(cos_sim)[:top_k]\n",
    "    print(top_k_array)\n",
    "    index_list = []\n",
    "    for item in top_k_array:\n",
    "        index_list.append(\n",
    "            list(item.keys())[0]\n",
    "        )\n",
    "    docs = []\n",
    "    for index in index_list:\n",
    "        docs.append(original_docs[index])\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '我最近一直流鼻涕，打喷嚏，头还痛，浑身乏力，这是怎么回事，怎么办啊？'\n",
    "docs = generate_docs(query,documents,3)\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc)\n",
    "\n",
    "prompt = f\"\"\"\n",
    "请根据我下面的文档内容，回答我提出的问题：\n",
    "文档内容：{docs}\n",
    "问题：{query}\n",
    "一律用中文回答\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler\n",
    "from sparkai.core.messages import ChatMessage\n",
    "\n",
    "#星火认知大模型Spark Max的URL值，其他版本大模型URL值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_URL = 'wss://spark-api.xf-yun.com/v3.5/chat'\n",
    "#星火认知大模型调用秘钥信息，请前往讯飞开放平台控制台（https://console.xfyun.cn/services/bm35）查看\n",
    "SPARKAI_APP_ID = ''\n",
    "SPARKAI_API_SECRET = ''\n",
    "SPARKAI_API_KEY = ''\n",
    "#星火认知大模型Spark Max的domain值，其他版本大模型domain值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_DOMAIN = 'generalv3.5'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = ChatSparkLLM(\n",
    "        spark_api_url=SPARKAI_URL,\n",
    "        spark_app_id=SPARKAI_APP_ID,\n",
    "        spark_api_key=SPARKAI_API_KEY,\n",
    "        spark_api_secret=SPARKAI_API_SECRET,\n",
    "        spark_llm_domain=SPARKAI_DOMAIN,\n",
    "        streaming=False,\n",
    "    )\n",
    "    messages = [ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=prompt\n",
    "    )]\n",
    "    handler = ChunkPrintHandler()\n",
    "    a = spark.generate([messages], callbacks=[handler])\n",
    "    print(a.generations[0][0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG based on embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = './bge-large-zh-v1.5/'\n",
    "from tqdm import tqdm\n",
    "from FlagEmbedding import FlagModel\n",
    "import numpy as np\n",
    "model = FlagModel(model_dir, \n",
    "                  query_instruction_for_retrieval=\"为这个句子生成表示以用于检索相关文章：\",\n",
    "                  use_fp16=True) # Setting use_fp16 to True speeds up computation with a slight performance degradation\n",
    "# similarity = embeddings_1 @ embeddings_2.T\n",
    "# print(similarity)\n",
    "\n",
    "# for s2p(short query to long passage) retrieval task, suggest to use encode_queries() which will automatically add the instruction to each query\n",
    "# corpus in retrieval task can still use encode() or encode_corpus(), since they don't need instruction\n",
    "# queries = ['query_1', 'query_2']\n",
    "# passages = [\"样例文档-1\", \"样例文档-2\"]\n",
    "# q_embeddings = model.encode_queries(queries)\n",
    "# p_embeddings = model.encode(passages)\n",
    "# scores = q_embeddings @ p_embeddings.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embedding_sentence(sentense):\n",
    "    embedding = model.encode(sentense)\n",
    "    return embedding\n",
    "def documents_embedding(file_path):\n",
    "    vec_documents = []\n",
    "    with open(file_path,'r') as file:\n",
    "        lines = file.readlines()\n",
    "    head = lines[0]\n",
    "    lines = lines[1:]\n",
    "    for line in tqdm(lines):\n",
    "        vec_documents.append(\n",
    "            embedding_sentence(line)\n",
    "        )\n",
    "    return np.array(vec_documents),lines,head\n",
    "\n",
    "vec_documents,original_docs,head = documents_embedding('./data.csv')\n",
    "print(vec_documents[0].shape)\n",
    "print(vec_documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "\n",
    "dimension = 1024\n",
    "db = faiss.IndexFlatL2(dimension)\n",
    "db.add(vec_documents)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## retrieve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def docs_generate(query,documents,top_k):\n",
    "    query_vector = model.encode_queries(query).reshape(1,-1)\n",
    "    distances, indices = db.search(query_vector, top_k)\n",
    "    print(f\"{indices[0]}: {distances[0]}\")\n",
    "    docs = []\n",
    "    for index in indices[0]:\n",
    "        docs.append(\n",
    "            documents[index]\n",
    "        )\n",
    "    return docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prompt combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k = 2\n",
    "query = '我最近一直流鼻涕，打喷嚏，头还痛，浑身乏力，这是怎么回事，怎么办啊？'\n",
    "docs = docs_generate(query,original_docs,top_k)\n",
    "print()\n",
    "\n",
    "for doc in docs:\n",
    "    print(doc,end='')\n",
    "\n",
    "prompt = f\"\"\"\n",
    "请根据我下面的文档内容，回答我提出的问题：\n",
    "文档内容：{docs}\n",
    "问题：{query}\n",
    "一律用中文回答\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sparkai.llm.llm import ChatSparkLLM, ChunkPrintHandler\n",
    "from sparkai.core.messages import ChatMessage\n",
    "\n",
    "#星火认知大模型Spark Max的URL值，其他版本大模型URL值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_URL = 'wss://spark-api.xf-yun.com/v3.5/chat'\n",
    "#星火认知大模型调用秘钥信息，请前往讯飞开放平台控制台（https://console.xfyun.cn/services/bm35）查看\n",
    "SPARKAI_APP_ID = ''\n",
    "SPARKAI_API_SECRET = ''\n",
    "SPARKAI_API_KEY = ''\n",
    "#星火认知大模型Spark Max的domain值，其他版本大模型domain值请前往文档（https://www.xfyun.cn/doc/spark/Web.html）查看\n",
    "SPARKAI_DOMAIN = 'generalv3.5'\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    spark = ChatSparkLLM(\n",
    "        spark_api_url=SPARKAI_URL,\n",
    "        spark_app_id=SPARKAI_APP_ID,\n",
    "        spark_api_key=SPARKAI_API_KEY,\n",
    "        spark_api_secret=SPARKAI_API_SECRET,\n",
    "        spark_llm_domain=SPARKAI_DOMAIN,\n",
    "        streaming=False,\n",
    "    )\n",
    "    messages = [ChatMessage(\n",
    "        role=\"user\",\n",
    "        content=prompt\n",
    "    )]\n",
    "    handler = ChunkPrintHandler()\n",
    "    a = spark.generate([messages], callbacks=[handler])\n",
    "    print(a.generations[0][0].text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag-chat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
